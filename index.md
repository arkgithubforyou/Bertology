
---------
## The state of BERTology
---
Summer Semester 2021<br/>
<!--- [Prof. Dr. Alexander Koller](http://www.coli.uni-saarland.de/~koller/)<br/> -->
Zhai Fangzhou <br/>
April 15th - July 22nd, Thursday 12:15–13:45, in [this](https://zoom.us/s/3679328022?pwd=TkYzSk5nNDdudCtPNnExUHVXTlEzQT09) zoom room<br/>, password [588149]
Master Seminar <br/>

---
### NOTE 
if you are registered, please vote here for a few irregular meeting slots in the weeks of May 13th, June 3rd and May 27th, before April 29th:
https://doodle.com/poll/9k37dyw3g44k23ud?utm_source=poll&utm_medium=link 

---
### Introduction
Models in the transformer family have been improving state of the art in various, if not all, NLP tasks. However, our understanding of the reason behind its success is still limited. The term _Bertology_ refers to the area of research that collects the investigations of the mechanism of these models, including how they work, what information they encode, etc., together with the methodologies emerged from these efforts.

This seminar provides an introduction to the state of Bertology. We start with the architecture of the transformer models. Later on, we discuss a number of aspects of Bertology: what syntactic / semantic information are encoded in these models; what these models know in terms of common sense knowledge; whether these giant models fully exploits all their parameters; the environmental concerns behind these large models, etc.. We conclude with the discussion of two meta-level questions: do these models _understand_ natural language, and what the next generation of general purpose representation for NLP could look like.

---
### Schedule
All registrations I received was qualified. If you did try to register and do not find your name here, please get in touch. The code for the assignment is attached to this repo, you may check in case of interest :)

|Date |Presenter|Paper|
|---  |---      |---|
|April 15|Zhai Fangzhou|(First Meeting)|
|April 29|Pin-Jie Lin| [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://www.aclweb.org/anthology/N19-1423/)|
|May 6|Katharina Stein| [What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models](https://www.aclweb.org/anthology/2020.tacl-1.3/)|
|May 13|Leonie Harter| [What’s in a Name? Are BERT Named Entity Representations just as Good for any other Name?](https://www.aclweb.org/anthology/2020.repl4nlp-1.24/)|
|May 20|Peilu Lin| [Language Models as Knowledge Bases?](https://www.aclweb.org/anthology/D19-1250/)|
|May 27|Pavle Markovic| [Information-Theoretic Probing with Minimum Description Length](https://www.aclweb.org/anthology/2020.emnlp-main.14.pdf)|
|June 3|Meng Li| [Linguistic Knowledge and Transferability of Contextual Representations](https://www.aclweb.org/anthology/N19-1112/)|
|June 10|Sangeet Sagar| [Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned](https://www.aclweb.org/anthology/P19-1580.pdf)|
|June 17|Axel Allén| [Energy and Policy Considerations for Deep Learning in NLP](https://www.aclweb.org/anthology/P19-1355/)|
|June 24|Annegret Janzso| [Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data](https://www.aclweb.org/anthology/2020.acl-main.463/)|
|July 1|Rricha Jalota| [Can a Fruit Fly Learn Word Embeddings?](https://openreview.net/forum?id=xfmSoxdxFCG)|
|July 8|Suruthai Noon Lywen Pokaratsiri Goldstein| [Revealing the Dark Secrets of BERT](https://www.aclweb.org/anthology/D19-1445/)|
|July 15|Nora Graichen|[Quantity doesn’t buy quality syntax with neural language models](https://www.aclweb.org/anthology/D19-1592.pdf)|

---



### Prerequisites
A good knowledge of both neural networks and computational linguistics is necessary for meaningful participation, which should be justified by finishing Professor Koller's [Computational Linguistics](https://coli-saar.github.io/cl20/) and Professor Klakow's [Neural Networks: Implementation and Application](https://teaching.lsv.uni-saarland.de/nnia/). However, all other convincing arguments are acceptable. 

---

### Registration
Please send an email to [Zhai Fangzhou](mailto:thearkforyou@gmail.com) **before April 1st 00:00 UST+1**, to include
- Your information: full name and matrikulation number.
- A brief, honest argument that you fulfil the prerequisites (e.g. **I've passed both courses.** or **I am familiar with both disciplines.** If you are taking some of them this semester and feel confident to at least pass, it is also ok).
- An ordered list of your prefered papers for presentation that includes at least 3 items. 

Registration is first come first serve for those who fulfil the prerequisites; late registrations will be considered if more places are available. Paper assignment will be performed by the Hungarian algorithm for registrations before April 1st (I will run it on April 2nd, no worry), and first come first serve afterwards.


---


### Evaluation
There are usually two goals behind a seminar: **(1)** familiarize, and hopefully interest the attendees with the topic, and in particular, not only the paper assigned to each of them; **(2)** improve the attendees' skills of presentation. Thus all attendees will need to 

- present a selected paper;
- write a brief summary (2-3 pages) of Bertology as a research direction at the end of the seminar.

and of course, attend regularly.

### Term Paper
There is in general no constraint on what you do other than relevance to the discipline. For example, you could
1. Write a survey on a relevant topic or
2. Raise a point and provide some empirical support or
3. Do a small project.

Please get in touch for a meeting to discuss your plan or ideas if you want to write a term paper.

---

### Contact
For questions, mail to: [Zhai Fangzhou](mailto:thearkforyou@gmail.com)

---

### List of Papers
**You are welcome to propose papers not in this list as well.**
#### The transformer models<br/>
1. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://www.aclweb.org/anthology/N19-1423/)<br/>
2. [The taxonomy of Transformers](https://arxiv.org/pdf/2003.08271.pdf)
  
#### What does BERT know about...<br/>
- Syntax.<br/>
3. [A Structural Probe for Finding Syntax in Word Representations](https://www.aclweb.org/anthology/N19-1419/)<br/>
- Semantics.<br/>
4. [What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models](https://www.aclweb.org/anthology/2020.tacl-1.3/)<br/>
5. [Do NLP Models Know Numbers? Probing Numeracy in Embeddings](https://www.aclweb.org/anthology/D19-1534/)<br/>
6. [What’s in a Name? Are BERT Named Entity Representations just as Good for any other Name?](https://www.aclweb.org/anthology/2020.repl4nlp-1.24/)<br/>
- World Knowledge<br/>
7. [Language Models as Knowledge Bases?](https://www.aclweb.org/anthology/D19-1250/)<br/>
  
#### How does BERT _really_ work
- Transferability<br/>
8. [Linguistic Knowledge and Transferability of Contextual Representations](https://www.aclweb.org/anthology/N19-1112/)<br/>
- Where and how to find information<br/>
9. [Revealing the Dark Secrets of BERT](https://www.aclweb.org/anthology/D19-1445/)<br/>
10. [Information-Theoretic Probing with Minimum Description Length](https://www.aclweb.org/anthology/2020.emnlp-main.14.pdf)<br/>
- Does BERT really need so many parameters?<br/>
11. [Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned](https://www.aclweb.org/anthology/P19-1580.pdf)<br/>
12. [Quantity doesn’t buy quality syntax with neural language models](https://www.aclweb.org/anthology/D19-1592.pdf)<br/>
  
#### How expensive is BERT?<br/>
13. [Energy and Policy Considerations for Deep Learning in NLP](https://www.aclweb.org/anthology/P19-1355/)<br/>

#### At the meta-level...
- Does huge models really _understand_ language?
14. [Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data](https://www.aclweb.org/anthology/2020.acl-main.463/)
- How will the next generation NLP representation look like?
15. [Can a Fruit Fly Learn Word Embeddings?](https://openreview.net/forum?id=xfmSoxdxFCG)

---
