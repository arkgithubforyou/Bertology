
---------
## The state of BERTology
---
Summer Semester 2021<br/>
<!--- [Prof. Dr. Alexander Koller](http://www.coli.uni-saarland.de/~koller/)<br/> -->
Zhai Fangzhou <br/>
April 15th - July 22nd, Thursday 12:15–13:45, Master Seminar<br/>
in [this](https://zoom.us/s/3679328022?pwd=TkYzSk5nNDdudCtPNnExUHVXTlEzQT09) zoom room, password [588149] <br/>

---
### NOTE 
The schedule has been updated according to the votes.


---
### Introduction
Models in the transformer family have been improving state of the art in various, if not all, NLP tasks. However, our understanding of the reason behind its success is still limited. The term _Bertology_ refers to the area of research that collects the investigations of the mechanism of these models, including how they work, what information they encode, etc., together with the methodologies emerged from these efforts.

This seminar provides an introduction to the state of Bertology. We start with the architecture of the transformer models. Later on, we discuss a number of aspects of Bertology: what syntactic / semantic information are encoded in these models; what these models know in terms of common sense knowledge; whether these giant models fully exploits all their parameters; the environmental concerns behind these large models, etc.. We conclude with the discussion of two meta-level questions: do these models _understand_ natural language, and what the next generation of general purpose representation for NLP could look like.

---
### Schedule
The code for the assignment is attached to this repo, you may check in case of interest :)
Reminder: the ACL anthology provides video recordings for the presentation of some recent papers, which should be useful.

|Date |Presenter|Paper|
|---  |---      |---|
|April 15|Zhai Fangzhou|(First Meeting)|
|April 29|Pin-Jie Lin| [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://www.aclweb.org/anthology/N19-1423/)|
|May 6|Katharina Stein| [What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models](https://www.aclweb.org/anthology/2020.tacl-1.3/)|
|May 20|Zhai Fangzhou| Probing Transformers for Script Knowledge|
|May 27|Pavle Markovic| [Information-Theoretic Probing with Minimum Description Length](https://www.aclweb.org/anthology/2020.emnlp-main.14.pdf)|
|June 10|Sangeet Sagar| [Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned](https://www.aclweb.org/anthology/P19-1580.pdf)|
|June 14|Leonie Harter| [What’s in a Name? Are BERT Named Entity Representations just as Good for any other Name?](https://www.aclweb.org/anthology/2020.repl4nlp-1.24/)|
|June 21, 1pm sharp [here](https://us02web.zoom.us/j/87178886458?pwd=N3hkZVh2ak5kdHJyRkVJOVdoV2crQT09#success) |Meng Li| [Linguistic Knowledge and Transferability of Contextual Representations](https://www.aclweb.org/anthology/N19-1112/)|
|June 24|Annegret Janzso| [Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data](https://www.aclweb.org/anthology/2020.acl-main.463/)|
|July 1|Rricha Jalota| [Can a Fruit Fly Learn Word Embeddings?](https://openreview.net/forum?id=xfmSoxdxFCG)|
|July 8|Suruthai Noon Lywen Pokaratsiri Goldstein| [Revealing the Dark Secrets of BERT](https://www.aclweb.org/anthology/D19-1445/)|
|July 15|Nora Graichen|[Quantity doesn’t buy quality syntax with neural language models](https://www.aclweb.org/anthology/D19-1592.pdf)|
|July 22, 1pm sharp|Axel Allén| [Energy and Policy Considerations for Deep Learning in NLP](https://www.aclweb.org/anthology/P19-1355/)|

---



### Prerequisites
A good knowledge of both neural networks and computational linguistics is necessary for meaningful participation, which should be justified by finishing Professor Koller's [Computational Linguistics](https://coli-saar.github.io/cl20/) and Professor Klakow's [Neural Networks: Implementation and Application](https://teaching.lsv.uni-saarland.de/nnia/). However, all other convincing arguments are acceptable. 

---

### Registration
Please send an email to [Zhai Fangzhou](mailto:thearkforyou@gmail.com) **before April 1st 00:00 UST+1**, to include
- Your information: full name and matrikulation number.
- A brief, honest argument that you fulfil the prerequisites (e.g. **I've passed both courses.** or **I am familiar with both disciplines.** If you are taking some of them this semester and feel confident to at least pass, it is also ok).
- An ordered list of your prefered papers for presentation that includes at least 3 items. 

Registration is first come first serve for those who fulfil the prerequisites; late registrations will be considered if more places are available. Paper assignment will be performed by the Hungarian algorithm for registrations before April 1st (I will run it on April 2nd, no worry), and first come first serve afterwards.


---


### Evaluation
There are usually two goals behind a seminar: **(1)** familiarize, and hopefully interest the attendees with the topic, and in particular, not only the paper assigned to each of them; **(2)** improve the attendees' skills of presentation. Thus all attendees will need to 

- present a selected paper;
- write a brief summary (2-3 pages) of Bertology as a research direction at the end of the seminar. Due **August 15th**.

and of course, attend regularly.

### Term Paper
There is in general no constraint on what you do other than relevance to the discipline. For example, you could
1. Write a survey on a relevant topic or
2. Raise a point and provide some empirical support or
3. Do a small project.

Please get in touch for a meeting to discuss your plan or ideas if you want to write a term paper. There is no hard constraint on the deadline, but we should agree on a schedule when we start.

---

### Contact
For questions, mail to: [Zhai Fangzhou](mailto:thearkforyou@gmail.com)

---
