
---------
## The state of BERTology
---
Summer Semester Semester 2021<br/>
<!--- [Prof. Dr. Alexander Koller](http://www.coli.uni-saarland.de/~koller/)<br/> -->
Zhai Fangzhou <br/>
Thursday 12:15–13:45, Seminarraum 1.12 C7.2<br/>
Master Seminar <br/>

---
### Introduction
Models in the transformer family have been improving state of the art in various, if not all, NLP tasks. However, our understanding of the reason behind its success is still limited. The term _Bertology_ refers to the area of research that collects the investigations of the mechanism of these models, including how they work, what information they encode, etc., together with the methodologies emerged from these efforts.

This seminar provides an introduction to the state of Bertology. We start with the architecture of the transformer models. Later on, we discuss a number of aspects of Bertology: what syntactic / semantic information are encoded in these models; what these models know in terms of common sense knowledge; whether these giant models fully exploits all their parameters; the environmental concerns behind these large models, etc.. We conclude with the discussion of two meta-level questions: do these models _understand_ natural language, and what the next generation of general purpose representation for NLP could look like.

---
### List of Papers

#### The transformer models<br/>
1. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://www.aclweb.org/anthology/N19-1423/)<br/>
2. [The taxonomy of Transformers](https://arxiv.org/pdf/2003.08271.pdf)
  
#### What does BERT know about...<br/>
- Syntax.<br/>
3. [A Structural Probe for Finding Syntax in Word Representations](https://www.aclweb.org/anthology/N19-1419/)<br/>
- Semantics.<br/>
4. [What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models](https://www.aclweb.org/anthology/2020.tacl-1.3/)<br/>
5. [Do NLP Models Know Numbers? Probing Numeracy in Embeddings](https://www.aclweb.org/anthology/D19-1534/)<br/>
6. [What’s in a Name? Are BERT Named Entity Representations just as Good for any other Name?](https://www.aclweb.org/anthology/2020.repl4nlp-1.24/)<br/>
- World Knowledge<br/>
7. [Language Models as Knowledge Bases?](https://www.aclweb.org/anthology/D19-1250/)<br/>
  
#### How does BERT _really_ work
- Transferability<br/>
8. [Linguistic Knowledge and Transferability of Contextual Representations](https://www.aclweb.org/anthology/N19-1112/)<br/>
- Where and how to find information<br/>
9. [Revealing the Dark Secrets of BERT](https://www.aclweb.org/anthology/D19-1445/)<br/>
10. [Information-Theoretic Probing with Minimum Description Length](https://www.aclweb.org/anthology/2020.emnlp-main.14.pdf)<br/>
- Does BERT really need so many parameters?<br/>
11. [Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned](https://www.aclweb.org/anthology/P19-1580.pdf)<br/>
12. [Quantity doesn’t buy quality syntax with neural language models](https://www.aclweb.org/anthology/D19-1592.pdf)<br/>
  
#### How expensive is BERT?<br/>
13. [Energy and Policy Considerations for Deep Learning in NLP](https://www.aclweb.org/anthology/P19-1355/)<br/>

#### At the meta-level...
- Does huge models really _understand_ language?
14. [Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data](https://www.aclweb.org/anthology/2020.acl-main.463/)
- How will the next generation NLP representation look like?
15. [Can a Fruit Fly Learn Word Embeddings?](https://openreview.net/forum?id=xfmSoxdxFCG)

---

### Prerequisites
A good knowledge of both neural networks and computational linguistics is necessary for meaningful participation, which should be justified by finishing Professor Koller's [Computational Linguistics](https://coli-saar.github.io/cl20/) and Professor Klakow's [Neural Networks: Implementation and Application](https://teaching.lsv.uni-saarland.de/nnia/). However, all other convincing arguments are acceptable. 

---

### Registration
Please send an email to [Zhai Fangzhou](mailto:thearkforyou@gmail.com) **before April 1st 00:00 UST+1**, to include.
- Your information: full name and matrikulation number.
- A brief, honest argument that you fulfil the prerequisites (e.g. **I've passed both courses.** or **I am familiar with both disciplines.**).
- An ordered list of your prefered papers for presentation that includes at least 3 items. 

---


### Evaluation
There are usually two goals behind a seminar: **(1)** familiarize, and hopefully interest the attendees with the topic, and in particular, not only the paper assigned to each of them; **(2)** improve the attendees' skills of presentation. Thus all attendees will need to 

- present a selected paper;
- write a brief summary (2-3 pages) of Bertology as a research direction at the end of the seminar.

and of course, attend regularly.

If you want to write a term paper, just get in touch.

---

### Contact
For questions, mail to: [Zhai Fangzhou](mailto:thearkforyou@gmail.com)
